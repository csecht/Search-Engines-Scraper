#!/usr/bin/env python3
"""
Multi_search is a command-line web search aggregator in Python,
derived from the Search-Engine-Scraper package from
https://github.com/tasos-py/Search-Engines-Scraper.

Non-redundant aggregate results from several general purpose search
engines are returned as URLs and their page titles to the Terminal and
an auto-named text file.

The intent is to provide easy comparisons among search engines, avoid
unintended search filtering, and automatically store results to file.

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
    See the GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program. If not, see https://www.gnu.org/licenses/.

See LICENCE file for additional licenses of repository components.
"""
__author__ = 'Craig Echt'
__copyright__ = 'Copyright (C) 2022 C.S. Echt'
__license__ = 'GNU General Public License'
__program_name__ = 'multi_search'
__project_url__ = 'https://github.com/csecht/Search-Engines-Scraper'
__version__ = '0.1.1'
__credits__ = 'Tasos M Adamopoulos (tasos-py) and Mario Vilas'
__dev_environment__ = 'Python 3.8'
__status__ = 'Development Status :: 1 - Alpha'


import argparse
import sys
import time
from datetime import datetime

import search_engines as se
from search_utils import files
from search_utils import get_user_agent

FileIt = files.results2file

# Assign user agents here.
# Those not listed below send requests in the http_client.py module
#   using the default static UA:
#   'search_engines/0.5 Repo: https://github.com/tasos-py/Search-Engines-Scraper'
#   which is also available from get_user_agent.tasos_agent().
#   Engines using Default UA: Aol, Ask, Quant, Torch, Yahoo.

#   Those that don't accept the default can use these randomly assigned ua.
#   All can use random firefox_ver() as user agent.
#   All but DGG can use static python_agent().
# bing_ua = get_user_agent.random_agent()
# dogpile_ua = get_user_agent.firefox_ver()
# google_ua = get_user_agent.firefox_ver()

DDG_UA = get_user_agent.tasos_agent()
# mojeek_ua = get_user_agent.random_agent()  # Occasionally raises HTTP 403
MOJEEK_UA = get_user_agent.python_agent()
startpage_ua = get_user_agent.firefox_ver()

# NOTE: Cannot use the site: specifier for Mojeek, raises 403 Forbidden.
#   But site: does work for M in SearchAggregator project, sometimes.
# Other Search terms that have failed:
# -site: in Dogpile > ERROR HTTP 400 Bad Request (Request header too long)

# TODO: Create MetaGer engine class.

# The duplicated result closest to end of the results list is the one
#   retained in the unique result list, so engine order here matters for
#   engine-specific reporting of unique results.
engines = {
    # 'B': se.Bing(bing_ua),
    # 'G': se.Google(google_ua),
    # 'Y': se.Yahoo(),
    # 'A': se.Ask(),
    # 'AOL': se.Aol(),
    # 'DP': se.Dogpile(dogpile_ua),
    'DDG': se.Duckduckgo(DDG_UA),
    'M': se.Mojeek(MOJEEK_UA),
    'SP': se.Startpage(startpage_ua),
    }

engine_names = {
    # 'A': 'Ask',
    # 'AOL': 'AOL',
    # 'B': 'Bing',
    # 'DP': 'Dogpile',
    # 'G': 'Google',
    # 'Y': 'Yahoo',
    'DDG': 'DuckDuckGo',
    'M': 'Mojeek',
    'SP': 'Startpage',
    }


def parse_args() -> None:
    """Allow handling of common command line arguments.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument('--about',
                        help='Provides description, version, GNU license',
                        action='store_true',
                        default=False)
    parser.add_argument('--usage',
                        help='Search syntax examples',
                        action='store_true',
                        default=False
                        )

    args = parser.parse_args()
    if args.about:
        print(__doc__)
        print(f'{"Author:".ljust(13)}', __author__)
        print(f'{"License:".ljust(13)}', __license__)
        print(f'{"Copyright:".ljust(13)}', __copyright__)
        print(f'{"Program:".ljust(13)}', __program_name__)
        print(f'{"url:".ljust(13)}', __project_url__)
        print(f'{"Version:".ljust(13)}', __version__)
        print(f'{"Credits:".ljust(13)}', __credits__)
        print(f'{"Dev Env:".ljust(13)}', __dev_environment__)
        print(f'{"Status:".ljust(13)}', __status__)
        print()
        sys.exit(0)

    if args.usage:
        print('USAGE: Just enter your search term at the prompt, no parameters needed.')
        print('Example search term syntax:\n'
              'puppies\nclasses edu\ndogs cats\n*ology\nRock * roll -and\n'
              '"jump the shark"\njump AND shark\n')
        print("Specifiers like site:gov, 'dogs AND cats', or 'dogs -cats'"
              " may not work as expected with some engines.")
        print('Results are printed to the Terminal and written to a file in the ResultsFiles folder.')
        sys.exit(0)


def search_this(search_term) -> None:
    """
    engine.search() return options:
    'host' is the base url, e.g. en.wikipedia.org
    'links' is the target link, e.g. https://en.wikipedia.org/wiki/Goat
    'titles' is the link page title.
    'text' is the full link description.
    """

    combined_results = []

    for _e in engines:
        # Limit each engine to ~20-30 results for more even sampling.
        if _e == 'DDG':
            results = engines[_e].search(search_term, pages=1)
        else:
            results = engines[_e].search(search_term, pages=2)

        links = results.links()
        titles = results.titles()

        # Prepend the engine tag to each result title.
        for i, title in enumerate(titles):
            titles[i] = f'({_e}) {title}'
        e_result = list(zip(links, titles))
        combined_results.extend(e_result)

        report_count = f'{len(links)} hits from {engine_names[_e]} ({_e})'
        print(report_count)
        FileIt(search_term, f'{report_count}\n')

    # Filter unique urls, saving the last redundant hit from combined_results,
    #   where last is determined by the engines.keys() order.
    unique_results = list({tup[:1]: tup for tup in combined_results}.values())

    uniq_res_msg = (f'Found {len(combined_results)} hits.\n\n'
                    f'There are {len(unique_results)} unique hits.\n')
    print(uniq_res_msg)
    FileIt(search_term, f'{uniq_res_msg}\n')

    for tag in engine_names:
        tag = f'({tag})'
        num_uniq_hit = len([hit for hit in unique_results if tag in hit[1]])
        tag_msg = f'{num_uniq_hit} unique hits from {tag}'
        print(tag_msg)
        files.results2file(search_term, f'{tag_msg}\n')

    # Need a brief delay before Terminal scrolls to last line of results.
    time.sleep(2)

    for _r in unique_results:
        result_msg = f'\n{_r[0]}\n{_r[1]}'
        print(result_msg)
        FileIt(search_term, f'{result_msg}\n')

    print(f'Results were written/appended to {FileIt(search_term, "")}')
    print(f'\n=============== END of {len(unique_results)} results ==================')
    FileIt(search_term,
           f'\n{"=" * 26} END of {len(unique_results)} results {"=" * 26}\n')


def main() -> None:
    """
    Print parameters and header information to Terminal and file.
    Run the search if no arguments given.
    """
    parse_args()
    term = input("\nEnter search term: ")
    term = term.replace(' ', '+')

    user_agents_used = (
        f'DuckDuckGo user agent: {DDG_UA}\n'
        f'Mojeek user agent: {MOJEEK_UA}\n'
        f'Startpage user agent: {startpage_ua}\n')
    print('\n', user_agents_used)

    file_header = (
        f'SEARCH TERM: {term}    TIME: {datetime.now().strftime("%x %X")}')
    FileIt(term, f'{file_header}\n\n')
    FileIt(term, f'{user_agents_used}\n')

    search_this(term)


if __name__ == "__main__":
    try:
        main()
    except (EOFError, KeyboardInterrupt):
        # Note: Ctrl-c may not work with PyCharm Terminal interpreter when
        #   waiting for Terminal input; Ctrl-d does work.
        print(' *** Keyboard interrupt by user has quit the program ***\n')
        sys.exit()
